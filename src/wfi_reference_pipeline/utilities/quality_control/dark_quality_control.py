import yaml
import logging
import numpy as np
from wfi_reference_pipeline.utilities.config_handler import get_quality_control_config
from wfi_reference_pipeline.constants import REF_TYPE_DARK


class DarkQualityControl:
    def __init__(self, rfp_dark):
        """
        Dark Quality Control (QC) class will take in a Reference File Pipeline (RFP) dark created object
        and import QC Reference Values from a QC yaml file. Various statistics or metrics on
        the data or data quality flags determined by the RFP are compared to the reference values that
        were written to the configuration file from the Roman Telescope Branch Data Base (RTBDB).

        rfp_dark: object
            Reference File Pipeline object created from ref_type data model and generated by the RFP
            after populate_data_model()
        config_path: str
            Path to the YAML configuration file.
        check_all: boolean; default = True
            Flag to perform all quality control checks.
        """
        self.rfp_dark = rfp_dark
        self.checks_results_dict = {}  # Empty dictionary of quality control functions performed
        self.dark_controls = get_quality_control_config(REF_TYPE_DARK)


    def do_checks(self):
        """
        Method to do checks only set to true in the config file and populate a dictionary with the checks that
        will be performed as the key and the value for that is true or false
        """
        for qc_method, do_check in self.dark_controls.items():
            if do_check:
                # Construct the method name from the check name
                # Get the method and call it if it exists
                method = getattr(self, qc_method, None)
                if callable(method):
                    method()

    def check_mean_dark_rate(self):
        """
        If this function or method is called by the flag set to true then perform this quality control check.

        Get the statistic or property from the RFP ref_type object and compare to the reference value for that check.
        Update the empty dictionary that has each
        """
        print("Executing check_mean_dark_rate")
        return
        rfp_dark_mean_rate = np.mean(self.rfp_dark.data_array)  # Assuming rfp_dark_data is a numpy array
        logging.info(
            f"Mean dark rate for detector {self.rfp_dark.meta_data['detector']} mode {self.rfp_dark.meta_data['mode']} is {rfp_dark_mean_rate:.3f} dn/s")

        ref_value = self.dark_qc_reference_dict['max_mean_dark_rate_reference_value']
        logging.info(f"Compared to reference value {ref_value} for detector {self.rfp_dark_meta['detector']}")

        if rfp_dark_mean_rate < ref_value:
            self.checks_results_dict['check_mean_dark_rate'] = True
        else:
            self.checks_results_dict['check_mean_dark_rate'] = False

        logging.info("Quality Control logging statement result for check performed]")

    def check_med_dark_rate(self):
        print("Executing check_med_dark_rate")

    def check_std_dark_rate(self):
        print("Executing check_std_dark_rate")

    def check_num_hot_pix(self):
        print("Executing check_num_hot_pix")

    def check_num_dead_pix(self):
        print("Executing check_num_dead_pix")

    def check_num_unreliable_pix(self):
        print("Executing check_num_unreliable_pix")

    def check_num_warm_pix(self):
        print("Executing check_num_warm_pix")






    def qc_checks_notifications(self):
        """
        If all check_results_dict passed True, then return True, then ok to deliver in automated pipeline
            We should get a logging statement confirming everything and a slack notification all good, green, passed
        If any check is False, then return False, halt or pause delivery
            We should get error statements and logging statements saying that something failed - specifically knowing
            additional information like which detector and what test exactly and by how much it failed.
            For a failure we need email to RFP  points of contact and a real noticeable slack notification with maybe
            even tagging stakeholders or pinging the whole channel

        #TODO In the future or to brainstorm, let's consider making this smarter. We will have tolerances away from the
        reference values and if say, one detector fails, we dont halt the delivery, but deliver the rest, we get emails
        and slack but it is not a total failure of delivery

        #TODO additionally we we make it smarter by checking against the tolerance too. If a detector fails but within
        some margin or tolerance then that is ok to deliver and notify to go back and update.

        #TODO WHAT WE DONT WANT WITH THESE TODOS
        We dont want to impede a delivery that wont break romancal and be enough that one person could do a manual submission
        to CRDS the next day or someone could. So we want to know as much as possible. Set the limitations on this though
        Scenario for yellow delivery - 1 detector fails, delivery 17, not all, notify and replace
        Scenario to avoid - ALL DETECTORs fail within tolerance, something would be up with data, dont deliver
        Sencario to consider yellow ok - 2 detectors fail within tolerance but not more than 2

        """


        qc_checks_all = True
        return True

    def send_results_to_rtdb(self):
        """
        Need method to write calculated values for the quality control checks to the RTBDB RFP tables

        """

    def Update_reference_table_in_rtbdb(self):
        """
        Need method to update reference values in rtbdb at some point if desired
        :return:
        """


